#!/usr/bin/env python3
"""
SCALE Input Deck Generator with Multiple Element Burns

This script generates SCALE/ORIGEN input decks with multiple fuel elements,
each having their own flux spectrum, power history, and material composition.

The script processes:
1. Flux data from MCNP tally output (JSON format)
2. Power/time data from ORIGEN cards generator
3. Material compositions from MCNP material cards
4. Generates a single SCALE .inp file with multiple build_lib and case sections

Usage:
    python generate_scale_input.py --flux-json flux_data.json --power-time origen_cards.txt
    python generate_scale_input.py --flux-json flux_data.json --power-time origen_cards.txt --year 2023
    python generate_scale_input.py --all-data --output multi_element_burnup.inp
"""

import json
import re
import argparse
import sys
import sqlite3
import os
import shutil
from pathlib import Path
from datetime import datetime
import logging
from typing import Dict, List, Tuple, Optional, Any

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ScaleInputGenerator:
    """Generate SCALE input decks with multiple element burns"""
    
    def __init__(self):
        self.flux_data = {}  # Dictionary of element_name: flux_values
        self.power_data = []  # List of power values
        self.time_data = []   # List of time values  
        self.material_compositions = {}  # Dictionary of element_name: composition
        self.element_count = 0
        self.scaled_power_data = []  # List of scaled power values per element
        self.total_core_plates = None  # Total number of plates in reactor core
        self.power_per_element = None  # Override power per element
        self.date_metadata = {}  # Store date information from ORIGEN cards
        self.default_material = {
            'u235': 12.500, 
            'u238': 50.792,  
            'al27': 32.464, 
            'si28': 4.579, 
            'si29': 0.241, 
            'si30': 0.164
        }
        
    def load_flux_data(self, json_file_path: str) -> bool:
        """Load flux data from JSON file generated by mcnp_parser"""
        try:
            with open(json_file_path, 'r') as f:
                raw_data = json.load(f)
            
            logger.info(f"Loading flux data from: {json_file_path}")
            
            # Process the flux dictionary
            for element_key, flux_info in raw_data.items():
                if isinstance(flux_info, list) and len(flux_info) >= 2:
                    # Format: [energies, fluxes, std_dev]
                    # We want the flux values (index 1) and exclude the last energy group
                    flux_values = flux_info[1]
                    if len(flux_values) >= 56:
                        # Take first 56 values (exclude 57th group)
                        self.flux_data[element_key] = flux_values[:56]
                        logger.debug(f"Loaded flux for {element_key}: {len(self.flux_data[element_key])} groups")
                    else:
                        logger.warning(f"Element {element_key} has insufficient flux groups: {len(flux_values)}")
                else:
                    logger.warning(f"Unexpected data format for element {element_key}")
            
            self.element_count = len(self.flux_data)
            logger.info(f"Successfully loaded flux data for {self.element_count} elements")
            return True
            
        except Exception as e:
            logger.error(f"Error loading flux data from {json_file_path}: {e}")
            return False
    
    def _extract_date_metadata(self, content: str) -> Dict[str, Any]:
        """Extract date metadata from ORIGEN cards file header"""
        metadata = {
            'first_date': None,
            'last_date': None,
            'date_range_str': 'unknown',
            'filename': None
        }
        
        try:
            # Look for date range in header comments
            date_range_match = re.search(r'# Date range: (.+?) to (.+?)\n', content)
            if date_range_match:
                metadata['first_date'] = date_range_match.group(1).strip()
                metadata['last_date'] = date_range_match.group(2).strip()
                
                # Extract years for range string
                first_year = metadata['first_date'][:4] if len(metadata['first_date']) >= 4 else metadata['first_date']
                last_year = metadata['last_date'][:4] if len(metadata['last_date']) >= 4 else metadata['last_date']
                
                if first_year == last_year:
                    metadata['date_range_str'] = first_year
                else:
                    metadata['date_range_str'] = f"{first_year}-{last_year}"
            else:
                # Try single date patterns
                start_match = re.search(r'# Start date: (.+?)\n', content)
                end_match = re.search(r'# End date: (.+?)\n', content)
                
                if start_match:
                    metadata['first_date'] = start_match.group(1).strip()
                    year = metadata['first_date'][:4] if len(metadata['first_date']) >= 4 else metadata['first_date']
                    metadata['date_range_str'] = f"from {year}"
                elif end_match:
                    metadata['last_date'] = end_match.group(1).strip()  
                    year = metadata['last_date'][:4] if len(metadata['last_date']) >= 4 else metadata['last_date']
                    metadata['date_range_str'] = f"to {year}"
            
        except Exception as e:
            logger.warning(f"Could not extract date metadata: {e}")
            
        return metadata
    
    def _generate_filename_with_date(self, base_name: str, extension: str = ".inp") -> str:
        """Generate filename with date information"""
        if self.date_metadata and self.date_metadata['date_range_str'] != 'unknown':
            return f"{base_name}_{self.date_metadata['date_range_str']}{extension}"
        else:
            return f"{base_name}{extension}"
    
    def get_date_info(self) -> Dict[str, Any]:
        """Get date metadata for external use"""
        return self.date_metadata.copy()
    
    def load_power_time_data(self, power_time_file: str) -> bool:
        """Load power and time data from ORIGEN cards file"""
        try:
            with open(power_time_file, 'r') as f:
                content = f.read()
            
            logger.info(f"Loading power/time data from: {power_time_file}")
            
            # Extract date metadata from file
            self.date_metadata = self._extract_date_metadata(content)
            self.date_metadata['filename'] = Path(power_time_file).name
            
            if self.date_metadata['first_date'] or self.date_metadata['last_date']:
                logger.info(f"Date range detected: {self.date_metadata['date_range_str']}")
            
            # Parse power values
            power_match = re.search(r'# POWER BLOCK \(MW\)\n(.*?)\n\n', content, re.DOTALL)
            if not power_match:
                logger.error("Could not find POWER BLOCK in file")
                return False
            
            power_text = power_match.group(1)
            self.power_data = [float(x) for x in power_text.split()]
            
            # Parse time values
            time_match = re.search(r'# TIME BLOCK \(minutes\)\n(.*?)(?:\n\n|$)', content, re.DOTALL)
            if not time_match:
                logger.error("Could not find TIME BLOCK in file")
                return False
            
            time_text = time_match.group(1)
            self.time_data = [int(x) for x in time_text.split()]
            
            if len(self.power_data) != len(self.time_data):
                logger.error(f"Mismatch between power ({len(self.power_data)}) and time ({len(self.time_data)}) data points")
                return False
            
            logger.info(f"Successfully loaded {len(self.power_data)} power/time data points")
            return True
            
        except Exception as e:
            logger.error(f"Error loading power/time data from {power_time_file}: {e}")
            return False
    
    def calculate_element_power(self) -> bool:
        """Calculate power per element based on total core configuration"""
        if not self.power_data:
            logger.error("No power data loaded - cannot calculate element power")
            return False
        
        try:
            if self.power_per_element is not None:
                # Manual override - scale based on maximum power value
                if max(self.power_data) > 0:
                    scaling_factor = self.power_per_element / max(self.power_data)
                else:
                    scaling_factor = 0
                logger.debug(f"Using manual power per element: {self.power_per_element} MW")
                
            elif self.total_core_plates is not None:
                # Scale based on total core plates
                num_simulated_elements = len(self.flux_data)
                if self.total_core_plates > 0 and num_simulated_elements > 0:
                    scaling_factor = num_simulated_elements / self.total_core_plates
                    logger.debug(f"Scaling power for {num_simulated_elements} elements out of {self.total_core_plates} total plates")
                else:
                    scaling_factor = 1.0
                    logger.warning("Invalid total core plates or element count, using full power")
                    
            else:
                # Default: divide equally among simulated elements
                num_simulated_elements = len(self.flux_data)
                if num_simulated_elements > 0:
                    scaling_factor = 1.0 / num_simulated_elements
                    logger.debug(f"Dividing power equally among {num_simulated_elements} elements")
                else:
                    scaling_factor = 1.0
                    logger.warning("No elements found, using full power")
            
            # Apply scaling to all power values
            self.scaled_power_data = [p * scaling_factor for p in self.power_data]
            
            logger.info(f"Power scaled by factor {scaling_factor:.6f}")
            return True
            
        except Exception as e:
            logger.error(f"Error calculating element power: {e}")
            return False
    
    def load_materials_from_database(self, db_path: str, cycle_number: int = None) -> bool:
        """Load material compositions from materials database"""
        if not Path(db_path).exists():
            logger.error(f"Database file not found: {db_path}")
            return False
        
        try:
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            
            # Get the latest cycle if cycle_number not specified
            if cycle_number is None:
                cursor.execute("SELECT MAX(cycle_number) FROM materials")
                result = cursor.fetchone()
                cycle_number = result[0] if result[0] is not None else 1
                logger.info(f"Using latest cycle from database: {cycle_number}")
            
            # Get all materials for the specified cycle
            cursor.execute("""
                SELECT m.element_name, m.total_mass_g, i.isotope_name, i.mass_grams
                FROM materials m
                JOIN isotopes i ON m.id = i.material_id
                WHERE m.cycle_number = ?
                ORDER BY m.element_name, i.mass_grams DESC
            """, (cycle_number,))
            
            results = cursor.fetchall()
            if not results:
                logger.warning(f"No materials found for cycle {cycle_number}")
                return False
            
            # Group by element name
            element_materials = {}
            current_element = None
            current_composition = {}
            current_total_mass = 0
            
            for element_name, total_mass_g, isotope_name, mass_grams in results:
                if element_name != current_element:
                    # Save previous element if it exists
                    if current_element is not None:
                        element_materials[current_element] = current_composition.copy()
                    
                    # Start new element
                    current_element = element_name
                    current_composition = {}
                    current_total_mass = total_mass_g
                
                # Convert isotope name to SCALE format
                scale_isotope = self.convert_to_scale_isotope(isotope_name)
                if scale_isotope:
                    current_composition[scale_isotope] = mass_grams
            
            # Don't forget the last element
            if current_element is not None:
                element_materials[current_element] = current_composition.copy()
            
            # Map element names to flux data keys
            self.material_compositions = {}
            db_elements_list = list(element_materials.keys())
            
            for i, flux_key in enumerate(self.flux_data.keys()):
                # Try different matching strategies
                matching_element = None
                
                # Strategy 1: Direct match
                if flux_key in element_materials:
                    matching_element = flux_key
                
                # Strategy 2: Simple index-based mapping for generic element names
                elif i < len(db_elements_list):
                    matching_element = db_elements_list[i]
                    logger.debug(f"Using index-based mapping: flux key '{flux_key}' -> database element '{matching_element}'")
                
                # Strategy 3: Partial string matching
                else:
                    for db_element in element_materials.keys():
                        if db_element in flux_key or flux_key in db_element:
                            matching_element = db_element
                            break
                
                if matching_element:
                    self.material_compositions[flux_key] = element_materials[matching_element]
                    logger.debug(f"Mapped flux key '{flux_key}' to database element '{matching_element}'")
                else:
                    # Use default if no match found
                    self.material_compositions[flux_key] = self.default_material.copy()
                    logger.warning(f"No database match for '{flux_key}', using defaults")
            
            conn.close()
            logger.info(f"Successfully loaded {len(element_materials)} material compositions from database (cycle {cycle_number})")
            return True
            
        except Exception as e:
            logger.error(f"Error loading materials from database: {e}")
            return False
    
    def convert_to_scale_isotope(self, isotope_name: str) -> Optional[str]:
        """Convert isotope name from database format to SCALE format"""
        # Database format: 'c-12', 'u-235', 'he-4', etc.
        # SCALE format: 'c12', 'u235', 'he4', etc.
        if '-' in isotope_name:
            parts = isotope_name.split('-')
            if len(parts) == 2:
                element, mass = parts
                return f"{element}{mass}"
        
        # If no conversion needed or format not recognized, return as-is
        return isotope_name
    
    def load_material_compositions(self, material_file: Optional[str] = None, materials_db: Optional[str] = None, cycle_number: int = None) -> bool:
        """Load material compositions from database, MCNP file, or use defaults"""
        
        # Priority 1: Database (if provided)
        if materials_db:
            if self.load_materials_from_database(materials_db, cycle_number):
                return True
            else:
                logger.warning("Failed to load from database, trying other sources")
        
        # Priority 2: MCNP material file (if provided)
        if material_file and Path(material_file).exists():
            try:
                with open(material_file, 'r') as f:
                    content = f.read()
                
                logger.info(f"Loading material compositions from: {material_file}")
                
                # Parse MCNP material cards
                # This is a simplified parser - may need enhancement for complex materials
                material_pattern = r'M(\d+)\s+nlib=\w+\s*(.*?)(?=\n[Mc]|\nend|\Z)'
                matches = re.findall(material_pattern, content, re.DOTALL | re.IGNORECASE)
                
                for mat_id, mat_content in matches:
                    # Parse isotope data - simplified approach
                    isotope_lines = [line.strip() for line in mat_content.split('\n') if line.strip() and not line.strip().startswith('!')]
                    
                    composition = {}
                    for line in isotope_lines:
                        parts = line.split()
                        if len(parts) >= 2:
                            try:
                                zaid = int(parts[0])
                                fraction = abs(float(parts[1]))  # Take absolute value of weight fraction
                                # Convert ZAID to isotope name (simplified)
                                isotope_name = self.zaid_to_isotope(zaid)
                                if isotope_name:
                                    composition[isotope_name] = fraction
                            except (ValueError, IndexError):
                                continue
                    
                    if composition:
                        material_name = f"Material_M{mat_id}"
                        self.material_compositions[material_name] = composition
                        logger.debug(f"Loaded material {material_name} with {len(composition)} isotopes")
                
                if self.material_compositions:
                    logger.info(f"Successfully loaded {len(self.material_compositions)} material compositions")
                    return True
                else:
                    logger.warning("No valid material compositions found, using defaults")
                    
            except Exception as e:
                logger.error(f"Error loading material compositions: {e}")
        
        # Priority 3: Use default composition for all elements
        logger.info("Using default fuel composition for all elements")
        for element_key in self.flux_data.keys():
            self.material_compositions[element_key] = self.default_material.copy()
        
        return True
    
    def zaid_to_isotope(self, zaid: int) -> Optional[str]:
        """Convert ZAID to isotope name (simplified mapping)"""
        zaid_map = {
            92235: 'u235', 92238: 'u238',
            8016: 'o16',
            6012: 'c12',
            14028: 'si28', 14029: 'si29', 14030: 'si30',
            2004: 'he4'
        }
        return zaid_map.get(zaid)
    
    def group_elements_by_assembly(self) -> Dict[str, List[str]]:
        """Group elements by their assembly name"""
        assemblies = {}
        for element_key in self.flux_data.keys():
            # Extract assembly name (e.g., "Assembly MTR-F-001" from "Assembly MTR-F-001, Element #1")
            if ',' in element_key:
                assembly_name = element_key.split(',')[0].strip()
            else:
                # Fallback for element keys without comma
                assembly_name = element_key.strip()
            
            if assembly_name not in assemblies:
                assemblies[assembly_name] = []
            assemblies[assembly_name].append(element_key)
        
        # Sort elements within each assembly by element number (not alphabetically)
        for assembly_name in assemblies:
            assemblies[assembly_name].sort(key=lambda x: self.extract_element_number(x))
        
        logger.info(f"Found {len(assemblies)} assemblies:")
        for assembly_name, elements in assemblies.items():
            logger.debug(f"  {assembly_name}: {len(elements)} elements")
        
        return assemblies
    
    def extract_element_number(self, element_key: str) -> int:
        """Extract element number from element key (e.g., 'Assembly MTR-F-001, Element #5' -> 5)"""
        try:
            # Look for pattern like "Element #5" or "Element 5"
            match = re.search(r'Element\s*#?(\d+)', element_key, re.IGNORECASE)
            if match:
                return int(match.group(1))
            
            # Fallback: try to find any number at the end
            match = re.search(r'(\d+)$', element_key.strip())
            if match:
                return int(match.group(1))
            
            # If no number found, return 1 as default
            logger.warning(f"Could not extract element number from '{element_key}', using 1")
            return 1
        except Exception as e:
            logger.warning(f"Error extracting element number from '{element_key}': {e}, using 1")
            return 1
    
    def validate_flux_data(self, element_key: str) -> bool:
        """
        Validate flux data for an element
        
        Args:
            element_key: Element identifier
            
        Returns:
            True if flux data is valid, False if all zeros or invalid
        """
        if element_key not in self.flux_data:
            return False
        
        flux_values = self.flux_data[element_key]
        
        # Check if all flux values are zero or near zero
        if not flux_values or all(abs(val) < 1e-15 for val in flux_values):
            return False
        
        # Check for any NaN or infinite values
        try:
            if any(not isinstance(val, (int, float)) or 
                   val != val or  # NaN check
                   abs(val) == float('inf') for val in flux_values):
                return False
        except (TypeError, ValueError):
            return False
        
        return True
    
    def generate_element_input(self, assembly_name: str, element_key: str, 
                             element_number: int, output_file: str) -> tuple:
        """
        Generate SCALE input file for a single element
        
        Note: Validation should be done before calling this method
        
        Returns:
            Tuple of (success: bool, skip_reason: str or None)
            - (True, None): Successfully generated
            - (False, None): Failed due to error
        """
        if not self.power_data or not self.time_data:
            logger.error("No power/time data loaded")
            return (False, None)  # This is a fatal error
        
        logger.info(f"Generating SCALE input for {element_key}: {output_file}")
        
        try:
            with open(output_file, 'w') as f:
                # Write header with assembly and element info
                f.write(f"'SCALE Burnup Analysis - {assembly_name}, Element #{element_number}\n")
                f.write("'Single Element Parallel Execution\n")
                f.write(f"'Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                
                # Write ORIGEN header (skip the original title line)
                header_lines = self.generate_origen_header().split('\n')
                for line in header_lines[1:]:  # Skip first line which is the title
                    f.write(line + '\n')
                f.write("\n")
                
                # Write build_lib section for this element (use element_number for unique flux file)
                f.write(self.generate_build_lib_for_element(element_key, element_number))
                f.write("\n")
                
                # Write case section for this element (use element_number for unique flux file)
                f.write(self.generate_case_for_element(element_key, element_number))
                f.write("\n")
                
                # Write end statement
                f.write("end\n")
            
            logger.info(f"Successfully generated {element_key} input file")
            return (True, None)
            
        except Exception as e:
            logger.error(f"Error writing {element_key} input file: {e}")
            return (False, None)
    
    def generate_origen_header(self) -> str:
        """Generate the ORIGEN options and solver header"""
        header = """'SCALE Multi-Element Burnup Analysis
=origen
options{
    digits=6
}
solver{
    type=CRAM
    opt{
        substeps=1
    }
}"""
        return header
    
    def generate_build_lib(self, element_key: str, lib_index: int) -> str:
        """Generate build_lib section for a specific element"""
        lib_name = f"element_{lib_index:03d}.f33"
        flux_values = self.flux_data[element_key]
        
        # Format flux values for SCALE input
        flux_formatted = []
        for i, flux_val in enumerate(flux_values):
            if i > 0 and i % 5 == 0:  # New line every 5 values
                flux_formatted.append(f"\n        {flux_val:.5E}")
            else:
                flux_formatted.append(f"{flux_val:.5E}")
        
        flux_string = ", ".join(flux_formatted)
        
        build_lib = f'''build_lib("{lib_name}"){{
% Build library for {element_key}
    decay{{
        type=ENDF_DECAY
        resource="${{DATA}}/origen_data/origen.rev03.decay.data"
    }}
    neutron(1){{
        type=ENDF_ENERGY_DEPENDENT
        reaction_resource="n56.reaction.data"
        spectrum {{
            type=MULTIGROUP
            flux=[{flux_string}]
        }}
    }}
}}'''
        return build_lib

    def generate_build_lib_for_element(self, element_key: str, element_number: int) -> str:
        """Generate build_lib section for a single element file using element-specific flux file"""
        lib_name = f"element{element_number:03d}.f33"
        flux_values = self.flux_data[element_key]
        
        # Format flux values for SCALE input
        flux_formatted = []
        for i, flux_val in enumerate(flux_values):
            if i > 0 and i % 5 == 0:  # New line every 5 values
                flux_formatted.append(f"\n        {flux_val:.5E}")
            else:
                flux_formatted.append(f"{flux_val:.5E}")
        
        flux_string = ", ".join(flux_formatted)
        
        build_lib = f'''build_lib("{lib_name}"){{
% Build library for {element_key}
    decay{{
        type=ENDF_DECAY
        resource="${{DATA}}/origen_data/origen.rev03.decay.data"
    }}
    neutron(1){{
        type=ENDF_ENERGY_DEPENDENT
        reaction_resource="n56.reaction.data"
        spectrum {{
            type=MULTIGROUP
            flux=[{flux_string}]
        }}
    }}
}}'''
        return build_lib
    
    def generate_case(self, element_key: str, case_index: int) -> str:
        """Generate case section for a specific element"""
        lib_name = f"element_{case_index:03d}.f33"
        case_name = f"element_{case_index:03d}_burn"
        
        # Get material composition
        if element_key in self.material_compositions:
            composition = self.material_compositions[element_key]
        else:
            composition = self.default_material
        
        # Format material composition
        mat_items = []
        for isotope, fraction in composition.items():
            mat_items.append(f"{isotope}={fraction:.9f}")
        mat_string = " ".join(mat_items)
        
        # Format power and time data
        power_formatted = []
        # Use scaled power data if available, otherwise fall back to original power data
        power_source = self.scaled_power_data if self.scaled_power_data else self.power_data
        for i, power in enumerate(power_source):
            if i > 0 and i % 10 == 0:  # New line every 10 values
                power_formatted.append(f"\n           {power:.5E}")
            else:
                power_formatted.append(f"{power:.5E}")
        power_string = "  ".join(power_formatted)
        
        time_formatted = []
        for i, time in enumerate(self.time_data):
            if i > 0 and i % 15 == 0:  # New line every 15 values
                time_formatted.append(f"\n        {time}")
            else:
                time_formatted.append(f"{time}")
        time_string = "  ".join(time_formatted)
        
        case = f'''case({case_name}){{
    title="{element_key} Irradiation"
% Material Comp for {element_key} in grams
    mat{{
        iso=[ {mat_string} ]
        units=grams
    }}
    lib{{
        file="{lib_name}"
        pos=1
    }}
% Power and time history
    time{{
        t=[{time_string}]
        units=minutes
    }}
    power=[{power_string}]
    print{{
        nuc{{
            total=yes
            units=GRAMS
        }}
    }}
    save=yes
}}'''
        return case

    def generate_case_for_element(self, element_key: str, element_number: int) -> str:
        """Generate case section for a single element file using element-specific flux file"""
        lib_name = f"element{element_number:03d}.f33"
        case_name = f"element{element_number:03d}_burn"
        
        # Get material composition
        if element_key in self.material_compositions:
            composition = self.material_compositions[element_key]
        else:
            composition = self.default_material
        
        # Format material composition
        mat_items = []
        for isotope, fraction in composition.items():
            mat_items.append(f"{isotope}={fraction:.9f}")
        mat_string = " ".join(mat_items)
        
        # Format power and time data
        power_formatted = []
        # Use scaled power data if available, otherwise fall back to original power data
        power_source = self.scaled_power_data if self.scaled_power_data else self.power_data
        for i, power in enumerate(power_source):
            if i > 0 and i % 10 == 0:  # New line every 10 values
                power_formatted.append(f"\n           {power:.5E}")
            else:
                power_formatted.append(f"{power:.5E}")
        power_string = "  ".join(power_formatted)
        
        time_formatted = []
        for i, time in enumerate(self.time_data):
            if i > 0 and i % 15 == 0:  # New line every 15 values
                time_formatted.append(f"\n        {time}")
            else:
                time_formatted.append(f"{time}")
        time_string = "  ".join(time_formatted)
        
        case = f'''case({case_name}){{
    title="{element_key} Irradiation"
% Material Comp for {element_key} in grams
    mat{{
        iso=[ {mat_string} ]
        units=grams
    }}
    lib{{
        file="{lib_name}"
        pos=1
    }}
% Power and time history
    time{{
        t=[{time_string}]
        units=minutes
    }}
    power=[{power_string}]
    print{{
        nuc{{
            total=yes
            units=GRAMS
        }}
    }}
    save=yes
}}'''
        return case
    
    def generate_assembly_input(self, assembly_name: str, elements: List[str], output_file: str) -> bool:
        """Generate SCALE input file for a single assembly with all its elements"""
        if not elements:
            logger.warning(f"No elements found for assembly {assembly_name}")
            return False
        
        if not self.power_data or not self.time_data:
            logger.error("No power/time data loaded")
            return False
        
        logger.info(f"Generating SCALE input for {assembly_name}: {output_file}")
        logger.debug(f"  Elements: {len(elements)}")
        
        try:
            with open(output_file, 'w') as f:
                # Write header
                f.write(f"'SCALE Burnup Analysis - {assembly_name}\n")
                f.write(self.generate_origen_header()[1:])  # Skip first line since we wrote custom title
                f.write("\n")
                
                # Write build_lib sections for this assembly's elements
                lib_index = 1
                for element_key in elements:
                    if element_key in self.flux_data:
                        f.write(self.generate_build_lib(element_key, lib_index))
                        f.write("\n")
                        lib_index += 1
                
                # Write case sections for this assembly's elements
                case_index = 1
                for element_key in elements:
                    if element_key in self.flux_data:
                        f.write(self.generate_case(element_key, case_index))
                        f.write("\n")
                        case_index += 1
                
                # Write end statement
                f.write("end\n")
            
            logger.info(f"Successfully generated {assembly_name} input: {len(elements)} elements")
            return True
            
        except Exception as e:
            logger.error(f"Error writing {assembly_name} input file: {e}")
            return False

    def generate_scale_input(self, output_file: str = "scale_multi_element.inp") -> bool:
        """Generate the complete SCALE input file"""
        if not self.flux_data:
            logger.error("No flux data loaded")
            return False
        
        if not self.power_data or not self.time_data:
            logger.error("No power/time data loaded")
            return False
        
        # Calculate scaled power per element
        if not self.calculate_element_power():
            logger.error("Failed to calculate element power scaling")
            return False
        
        logger.info(f"Generating SCALE input file: {output_file}")
        
        try:
            with open(output_file, 'w') as f:
                # Write header
                f.write(self.generate_origen_header())
                f.write("\n")
                
                # Write build_lib sections
                for i, element_key in enumerate(self.flux_data.keys(), 1):
                    f.write(self.generate_build_lib(element_key, i))
                    f.write("\n")
                
                # Write case sections
                for i, element_key in enumerate(self.flux_data.keys(), 1):
                    f.write(self.generate_case(element_key, i))
                    f.write("\n")
                
                # Write end statement
                f.write("end\n")
            
            logger.info(f"Successfully generated SCALE input file: {output_file}")
            logger.info(f"Elements processed: {len(self.flux_data)}")
            logger.info(f"Power/time points: {len(self.power_data)}")
            
            return True
            
        except Exception as e:
            logger.error(f"Error writing SCALE input file: {e}")
            return False

def generate_helper_scripts(output_dir: Path, assemblies: Dict[str, List[str]], mode: str = 'assembly'):
    """Generate Python-based helper scripts for parallel execution"""
    logger.info(f"Generating Python helper scripts for {mode}-based parallel execution")
    
    # Copy the utility modules to the output directory
    tools_dir = Path(__file__).parent / "tools"
    
    # Copy scale_msg_parser.py
    msg_parser_src = tools_dir / "scale_msg_parser.py"
    msg_parser_dst = output_dir / "scale_msg_parser.py"
    if msg_parser_src.exists():
        import shutil
        shutil.copy2(msg_parser_src, msg_parser_dst)
    
    # Copy scale_parallel_runner.py
    runner_src = tools_dir / "scale_parallel_runner.py"
    runner_dst = output_dir / "scale_parallel_runner.py"
    if runner_src.exists():
        import shutil
        shutil.copy2(runner_src, runner_dst)
    
    # Copy monitor_status.py
    monitor_src = tools_dir / "monitor_status.py"
    monitor_dst = output_dir / "monitor_status.py"
    if monitor_src.exists():
        import shutil
        shutil.copy2(monitor_src, monitor_dst)
    
    # Generate list file for reference
    if mode == 'element':
        list_file = output_dir / "element_list.txt"
        list_header = "# Element List for Parallel SCALE Execution"
        total_elements = sum(len(elements) for elements in assemblies.values())
        total_line = f"# Total Elements: {total_elements}"
    else:
        list_file = output_dir / "assembly_list.txt"
        list_header = "# Assembly List for Parallel SCALE Execution"
        total_line = f"# Total Assemblies: {len(assemblies)}"
        
    with open(list_file, 'w') as f:
        f.write(list_header + "\n")
        f.write(f"# Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(total_line + "\n\n")
        
        if mode == 'element':
            for assembly_name, elements in assemblies.items():
                for element_key in elements:
                    # Create a temporary ScaleInputGenerator instance to use the extract_element_number method
                    temp_generator = ScaleInputGenerator()
                    element_number = temp_generator.extract_element_number(element_key)
                    safe_assembly = assembly_name.replace(' ', '_').replace('/', '-')
                    f.write(f"element_{element_number:03d}\t{assembly_name}\t{element_key}\n")
        else:
            for assembly_name, elements in assemblies.items():
                safe_name = assembly_name.replace(' ', '_').replace('/', '-')
                f.write(f"{safe_name}\t{len(elements)} elements\t{assembly_name}\n")
    
    # Generate Python parallel execution script
    run_script = output_dir / "run_parallel.py"
    with open(run_script, 'w') as f:
        f.write("#!/usr/bin/env python3\n")
        f.write('"""\n')
        f.write("SCALE Parallel Execution Runner\n")
        f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        if mode == 'element':
            total_jobs = sum(len(elements) for elements in assemblies.values())
            f.write(f"Total elements: {total_jobs}\n")
        else:
            f.write(f"Total assemblies: {len(assemblies)}\n")
        f.write('"""\n\n')
        
        f.write("import sys\n")
        f.write("import argparse\n")
        f.write("from pathlib import Path\n")
        f.write("from scale_parallel_runner import ScaleParallelRunner\n\n")
        
        f.write("def main():\n")
        if mode == 'element':
            f.write('    parser = argparse.ArgumentParser(description="Run SCALE elements in parallel")\n')
        else:
            f.write('    parser = argparse.ArgumentParser(description="Run SCALE assemblies in parallel")\n')
        f.write('    parser.add_argument("--workers", "-w", type=int, default=8,\n')
        f.write('                       help="Number of parallel workers (default: 8)")\n')
        f.write('    parser.add_argument("--scale-cmd", default="scalerte",\n')
        f.write('                       help="SCALE command to run (default: scalerte)")\n')
        f.write('    parser.add_argument("--verbose", "-v", action="store_true",\n')
        f.write('                       help="Enable verbose logging")\n')
        f.write('    \n')
        f.write('    args = parser.parse_args()\n')
        f.write('    \n')
        f.write('    # Create runner\n')
        f.write('    runner = ScaleParallelRunner(max_workers=args.workers, scale_command=args.scale_cmd)\n')
        f.write('    \n')
        f.write('    try:\n')
        f.write('        # Add jobs from current directory\n')
        if mode == 'element':
            f.write('        num_jobs = runner.add_jobs_from_directory(Path("."), "element_*.inp")\n')
            f.write('        print(f"Found {num_jobs} element jobs to run")\n')
        else:
            f.write('        num_jobs = runner.add_jobs_from_directory(Path("."), "assembly_*.inp")\n')
            f.write('        print(f"Found {num_jobs} assembly jobs to run")\n')
        f.write('        \n')
        f.write('        # Progress callback\n')
        f.write('        def progress_update(completed, total, job_name, success):\n')
        f.write('            status = "✅" if success else "❌"\n')
        f.write('            print(f"{status} {job_name} ({completed}/{total})")\n')
        f.write('        \n')
        f.write('        # Run jobs\n')
        f.write('        print(f"Starting parallel execution with {args.workers} workers...")\n')
        f.write('        print("Monitor progress with: python monitor_status.py")\n')
        f.write('        results = runner.run_all_parallel(progress_callback=progress_update)\n')
        f.write('        \n')
        f.write('        # Summary\n')
        f.write('        successful = sum(results.values())\n')
        f.write('        print(f"\\nExecution Summary:")\n')
        f.write('        print(f"  Total jobs: {len(results)}")\n')
        f.write('        print(f"  Successful: {successful}")\n')
        f.write('        print(f"  Failed: {len(results) - successful}")\n')
        f.write('        \n')
        f.write('        if successful < len(results):\n')
        f.write('            print(f"\\nFailed jobs:")\n')
        f.write('            for job_name, success in results.items():\n')
        f.write('                if not success:\n')
        f.write('                    print(f"  - {job_name}")\n')
        f.write('        \n')
        f.write('        sys.exit(0 if successful == len(results) else 1)\n')
        f.write('        \n')
        f.write('    except KeyboardInterrupt:\n')
        f.write('        print("\\nCanceling jobs...")\n')
        f.write('        runner.cancel_all()\n')
        f.write('        sys.exit(1)\n')
        f.write('    except Exception as e:\n')
        f.write('        print(f"Fatal error: {e}")\n')
        f.write('        sys.exit(1)\n')
        f.write('\n')
        f.write('if __name__ == "__main__":\n')
        f.write('    main()\n')
    
    # Make script executable
    os.chmod(run_script, 0o755)
    
    # Generate README with Python-based instructions
    readme_file = output_dir / "README.md"
    with open(readme_file, 'w') as f:
        f.write("# SCALE Parallel Execution (Python-based)\n\n")
        f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        if mode == 'element':
            total_elements = sum(len(elements) for elements in assemblies.values())
            f.write(f"Total Elements: {total_elements} (from {len(assemblies)} assemblies)\n\n")
        else:
            f.write(f"Total Assemblies: {len(assemblies)}\n\n")
        
        f.write("## Files Generated\n\n")
        if mode == 'element':
            for assembly_name, elements in assemblies.items():
                f.write(f"### {assembly_name}\n")
                for element_key in elements:
                    # Create a temporary ScaleInputGenerator instance to use the extract_element_number method
                    temp_generator = ScaleInputGenerator()
                    element_number = temp_generator.extract_element_number(element_key)
                    f.write(f"- `element_{element_number:03d}.inp`: {element_key}\n")
                f.write("\n")
        else:
            for assembly_name, elements in assemblies.items():
                safe_name = assembly_name.replace(' ', '_').replace('/', '-')
                f.write(f"- `assembly_{safe_name}.inp`: {len(elements)} elements from {assembly_name}\n")
        
        f.write("\n## Python Execution Scripts\n\n")
        f.write("- `run_parallel.py`: Main parallel execution script with .msg file monitoring\n")
        f.write("- `monitor_status.py`: Real-time job status monitoring\n")
        f.write("- `scale_msg_parser.py`: SCALE .msg file parser utility\n")
        f.write("- `scale_parallel_runner.py`: Parallel execution engine\n")
        
        f.write("\n## Execution Instructions\n\n")
        f.write("1. **Start parallel execution:**\n")
        f.write("   ```bash\n")
        f.write("   python run_parallel.py --workers 8    # Run with 8 parallel jobs\n")
        f.write("   python run_parallel.py --verbose      # Enable verbose logging\n")
        f.write("   ```\n\n")
        
        f.write("2. **Monitor progress (in separate terminal):**\n")
        f.write("   ```bash\n")
        f.write("   python monitor_status.py              # Real-time dashboard\n")
        f.write("   python monitor_status.py --once       # Single status check\n")
        f.write("   python monitor_status.py --refresh 2  # Update every 2 seconds\n")
        f.write("   ```\n\n")
        
        f.write("3. **Check results after completion:**\n")
        f.write("   ```bash\n")
        f.write("   python ../tools/collect_assembly_results.py --input-dir . --check-msg\n")
        f.write("   ```\n\n")
        
        f.write("## Key Features\n\n")
        f.write("- **Accurate completion detection** via .msg file parsing\n")
        f.write("- **Cross-platform** Python instead of bash\n")
        f.write("- **Real-time monitoring** with status dashboard\n")
        f.write("- **Robust error handling** with return code checking\n")
        f.write("- **Progress tracking** with runtime estimates\n")
        
        f.write("\n## Status Detection\n\n")
        f.write("The system monitors .msg files for completion messages like:\n")
        f.write("```\n")
        f.write("Scale job C:\\path\\to\\assembly.inp is finished.\n")
        f.write("Output is stored in C:\\path\\to\\assembly.out\n")
        f.write("Process finished with 0 return code; ran in 123 secs\n")
        f.write("```\n\n")
        f.write("**Note**: Return code 0 indicates SUCCESS (job completed successfully).\n")
        f.write("Non-zero return codes indicate failures or errors.\n\n")
        
        f.write("## Performance Benefits\n\n")
        total_elements = sum(len(elements) for elements in assemblies.values())
        f.write(f"- **Original**: 1 file with all {total_elements} elements (single-threaded)\n")
        if mode == 'element':
            f.write(f"- **Parallel**: {total_elements} elements running simultaneously (maximum parallelization)\n")
            f.write(f"- **Expected speedup**: ~{min(total_elements, 64)}x on {min(total_elements, 64)}+ core systems\n")
            f.write(f"- **Better fault tolerance**: Individual element failures don't affect others\n")
            f.write(f"- **Assembly tracking**: Each element file tracks its source assembly\n")
        else:
            f.write(f"- **Parallel**: {len(assemblies)} assemblies running simultaneously\n")
            f.write(f"- **Expected speedup**: ~{len(assemblies)}x on {len(assemblies)}+ core systems\n")
            f.write("- **Better fault tolerance**: Individual assembly failures don't affect others\n")
    
    logger.info(f"Generated Python helper scripts in {output_dir}:")
    logger.info(f"  - run_parallel.py: Main execution script")
    logger.info(f"  - monitor_status.py: Status monitoring dashboard")  
    logger.info(f"  - scale_msg_parser.py: .msg file parser")
    logger.info(f"  - scale_parallel_runner.py: Parallel execution engine")
    logger.info(f"  - README.md: Instructions")
    if mode == 'element':
        logger.info(f"  - element_list.txt: Element-to-assembly mapping")
    else:
        logger.info(f"  - assembly_list.txt: Assembly reference")

def main():
    parser = argparse.ArgumentParser(
        description="Generate SCALE input decks with multiple element burns",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Single large file (default)
  python generate_scale_input.py --flux-json flux_data.json --power-time origen_cards.txt
  
  # Per-assembly files for parallel execution  
  python generate_scale_input.py --flux-json flux_data.json --power-time origen_cards.txt --split-by-assembly
  
  # Per-element files for maximum parallel execution (RECOMMENDED)
  python generate_scale_input.py --flux-json flux_data.json --power-time origen_cards.txt --split-by-element
  
  # Specific assemblies only (works with both splitting modes)
  python generate_scale_input.py --flux-json flux_data.json --power-time origen_cards.txt --split-by-element --assemblies "MTR-F-001,MTR-F-005"
        """
    )
    
    parser.add_argument("--flux-json", required=True,
                        help="JSON file containing flux data from MCNP parser")
    
    parser.add_argument("--power-time", required=True,
                        help="File containing power/time data from ORIGEN cards generator")
    
    parser.add_argument("--material", 
                        help="MCNP material cards file (optional, uses defaults if not provided)")
    
    parser.add_argument("--materials-db", 
                        help="Materials database from parseOutput.py (takes priority over --material)")
    
    parser.add_argument("--cycle", type=int,
                        help="Cycle number to load from database (defaults to latest)")
    
    parser.add_argument("--output", "-o", default="scale_multi_element.inp",
                        help="Output SCALE input file name (default: scale_multi_element.inp)")
    
    parser.add_argument("--total-core-plates", type=int,
                        help="Total number of fuel plates in reactor core (for power scaling)")
    
    parser.add_argument("--power-per-element", type=float,
                        help="Override with specific power per element (MW)")
    
    parser.add_argument("--verbose", "-v", action="store_true",
                        help="Enable verbose logging")
    
    parser.add_argument("--split-by-assembly", action="store_true",
                        help="Generate separate input files per assembly for parallel execution")
    
    parser.add_argument("--split-by-element", action="store_true",
                        help="Generate separate input files per element for maximum parallel execution (recommended for large cases)")
    
    parser.add_argument("--output-dir", 
                        help="Output directory for parallel files (default: scale_runs/YYYY-MM-DD_HH-MM-SS)")
    
    parser.add_argument("--assemblies", 
                        help="Comma-separated list of specific assemblies to process (applies to both element and assembly splitting)")
    
    parser.add_argument("--elements", 
                        help="Comma-separated list of specific elements to process (e.g., 'Element #1,Element #5')")
    
    args = parser.parse_args()
    
    # Set logging level
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # Validate split options
    if args.split_by_assembly and args.split_by_element:
        logger.error("Cannot use both --split-by-assembly and --split-by-element at the same time")
        sys.exit(1)
    
    # Validate input files
    if not Path(args.flux_json).exists():
        logger.error(f"Flux JSON file not found: {args.flux_json}")
        sys.exit(1)
    
    if not Path(args.power_time).exists():
        logger.error(f"Power/time file not found: {args.power_time}")
        sys.exit(1)
    
    if args.material and not Path(args.material).exists():
        logger.error(f"Material file not found: {args.material}")
        sys.exit(1)
    
    # Create generator and process data
    try:
        generator = ScaleInputGenerator()
        
        # Set power scaling parameters
        generator.total_core_plates = args.total_core_plates
        generator.power_per_element = args.power_per_element
        
        # Load all required data
        if not generator.load_flux_data(args.flux_json):
            logger.error("Failed to load flux data")
            sys.exit(1)
        
        if not generator.load_power_time_data(args.power_time):
            logger.error("Failed to load power/time data")
            sys.exit(1)
        
        if not generator.load_material_compositions(args.material, args.materials_db, args.cycle):
            logger.error("Failed to load material compositions")
            sys.exit(1)
        
        # Calculate scaled power per element (needed for both modes)
        if not generator.calculate_element_power():
            logger.error("Failed to calculate element power scaling")
            sys.exit(1)
        
        # Choose generation mode
        if args.split_by_assembly or args.split_by_element:
            # Parallel file generation (either assembly-based or element-based)
            assemblies = generator.group_elements_by_assembly()
            
            # Filter assemblies if specific ones requested
            if args.assemblies:
                requested_assemblies = [a.strip() for a in args.assemblies.split(',')]
                assemblies = {name: elements for name, elements in assemblies.items() 
                            if any(req in name for req in requested_assemblies)}
                logger.info(f"Filtering to requested assemblies: {list(assemblies.keys())}")
            
            # Filter elements if specific ones requested (only for element splitting)
            if args.split_by_element and args.elements:
                requested_elements = [e.strip() for e in args.elements.split(',')]
                for assembly_name in assemblies:
                    assemblies[assembly_name] = [elem for elem in assemblies[assembly_name] 
                                               if any(req in elem for req in requested_elements)]
                # Remove assemblies with no elements after filtering
                assemblies = {name: elements for name, elements in assemblies.items() if elements}
                logger.info(f"Filtering to requested elements")
            
            if not assemblies:
                logger.error("No assemblies/elements found to process")
                sys.exit(1)
            
            # Create output directory
            if args.output_dir:
                output_dir = Path(args.output_dir)
            else:
                timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
                # Add date range info to directory name if available
                date_info = generator.get_date_info()
                if date_info and date_info.get('date_range_str', 'unknown') != 'unknown':
                    output_dir = Path(f"scale_runs/{timestamp}_{date_info['date_range_str']}")
                else:
                    output_dir = Path(f"scale_runs/{timestamp}")
            
            output_dir.mkdir(parents=True, exist_ok=True)
            
            if args.split_by_element:
                # Element-based file generation for maximum parallel execution
                logger.info(f"Creating element files in: {output_dir}")
                
                element_files = []
                element_mapping = {}
                skipped_elements = []
                global_element_counter = 0  # Global counter for successfully generated elements only
                processed_elements = 0  # Total elements attempted
                
                for assembly_name, elements in assemblies.items():
                    safe_assembly = assembly_name.replace(' ', '_').replace('/', '-')
                    
                    for element_key in elements:
                        local_element_number = generator.extract_element_number(element_key)  # For filename only
                        processed_elements += 1
                        
                        # Create filename: element_Assembly_MTR-F-001_E001.inp (using local element number)
                        output_file = output_dir / f"element_{safe_assembly}_E{local_element_number:03d}.inp"
                        
                        # First check if element is valid before incrementing counter
                        # Pre-validate the element
                        if element_key not in generator.flux_data:
                            skipped_elements.append({
                                'element_key': element_key,
                                'assembly': assembly_name,
                                'reason': 'no_flux_data'
                            })
                            logger.warning(f"Skipping {element_key}: no flux data found")
                            continue
                        
                        if not generator.validate_flux_data(element_key):
                            skipped_elements.append({
                                'element_key': element_key,
                                'assembly': assembly_name,
                                'reason': 'invalid_flux'
                            })
                            logger.warning(f"Skipping {element_key}: flux data is all zeros or invalid")
                            continue
                        
                        # Only increment counter for valid elements that will be generated
                        global_element_counter += 1
                        
                        # Use global counter for flux file reference
                        success, skip_reason = generator.generate_element_input(assembly_name, element_key, 
                                                                              global_element_counter, str(output_file))
                        
                        if success:
                            element_files.append(str(output_file))
                            
                            # Track mapping for result analysis
                            element_mapping[output_file.name] = {
                                'assembly': assembly_name,
                                'element_key': element_key,
                                'local_element_number': local_element_number,  # Element number within assembly
                                'global_element_number': global_element_counter,  # Global unique number for flux file
                                'safe_assembly': safe_assembly
                            }
                        else:
                            # Actual generation error - this is fatal (shouldn't happen after pre-validation)
                            logger.error(f"Failed to generate input for {element_key} after validation")
                            sys.exit(1)
                
                logger.info(f"Successfully generated {len(element_files)} element input files")
                logger.info(f"Total elements attempted: {processed_elements}")
                logger.info(f"Elements skipped: {len(skipped_elements)}")
                logger.info(f"Elements distributed across {len(assemblies)} assemblies")
                
                # Log details about skipped elements
                if skipped_elements:
                    logger.warning(f"Skipped elements summary:")
                    skip_reasons = {}
                    for skipped in skipped_elements:
                        reason = skipped['reason']
                        if reason not in skip_reasons:
                            skip_reasons[reason] = []
                        skip_reasons[reason].append(f"{skipped['assembly']} - {skipped['element_key']}")
                    
                    for reason, elements in skip_reasons.items():
                        logger.warning(f"  {reason}: {len(elements)} elements")
                        for element in elements[:3]:  # Show first 3 examples
                            logger.warning(f"    - {element}")
                        if len(elements) > 3:
                            logger.warning(f"    ... and {len(elements) - 3} more")
                
                # Save element mapping for tracking
                mapping_file = output_dir / "element_mapping.json"
                mapping_data = {
                    'generated_elements': element_mapping,
                    'skipped_elements': skipped_elements,
                    'summary': {
                        'total_attempted': processed_elements,
                        'successfully_generated': len(element_files),
                        'skipped': len(skipped_elements),
                        'assemblies': len(assemblies)
                    }
                }
                with open(mapping_file, 'w') as f:
                    json.dump(mapping_data, f, indent=2)
                logger.info(f"Element mapping saved to: {mapping_file}")
                
                # Generate helper scripts for element-based parallel execution
                generate_helper_scripts(output_dir, assemblies, mode='element')
                
            else:
                # Assembly-based file generation for parallel execution
                logger.info(f"Creating assembly files in: {output_dir}")
                
                assembly_files = []
                for assembly_name, elements in assemblies.items():
                    # Convert assembly name to filename-safe format
                    safe_name = assembly_name.replace(' ', '_').replace('/', '-')
                    output_file = output_dir / f"assembly_{safe_name}.inp"
                    
                    if generator.generate_assembly_input(assembly_name, elements, str(output_file)):
                        assembly_files.append(str(output_file))
                    else:
                        logger.error(f"Failed to generate input for {assembly_name}")
                        sys.exit(1)
                
                logger.info(f"Successfully generated {len(assembly_files)} assembly input files")
                logger.info(f"Total elements processed: {sum(len(elements) for elements in assemblies.values())}")
                
                # Generate helper scripts for assembly-based parallel execution
                generate_helper_scripts(output_dir, assemblies, mode='assembly')
            
        else:
            # Single large file generation (original mode)
            # Use date-based filename if using default output
            output_file = args.output
            if args.output == "scale_multi_element.inp":  # Default filename
                output_file = generator._generate_filename_with_date("scale_multi_element")
                logger.info(f"Using date-based filename: {output_file}")
            
            if not generator.generate_scale_input(output_file):
                logger.error("Failed to generate SCALE input file")
                sys.exit(1)
            logger.info("SCALE input generation completed successfully!")
        
    except Exception as e:
        logger.error(f"Fatal error: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()